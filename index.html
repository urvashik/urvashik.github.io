<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Urvashi Khandelwal Stanford Homepage</title>
  
  <meta name="author" content="Urvashi Khandelwal">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="images/nlp-logo.jpeg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Urvashi Khandelwal</name>
              </p>
              <p> 
              I am a Research Scientist at <a href=https://ai.google/research/>Google AI</a>, where I work on natural language processing and machine learning. My research interests lie in understanding and improving the generalization capabilities of language models and sequence generation models, and building systems that can adapt to new tasks and changing data distributions.
              </p>
              <p>

              I received my PhD in Computer Science at Stanford University, where I worked with the <a href=https://nlp.stanford.edu/>Stanford Natural Language Processing Group</a> and was advised by <a href=https://web.stanford.edu/~jurafsky/>Professor Dan Jurafsky</a>. Prior to that, I was an undergraduate student at the University of Illinois Urbana-Champaign, and did some research in information network analysis with <a href=http://hanj.cs.illinois.edu/>Professor Jiawei Han</a>.</br>             
              </p>
              <p style="text-align:left">
                Email: urvashik@stanford.edu</br> 
                <a href="https://twitter.com/ukhndlwl">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/urvashik">Github</a>  &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?view_op=list_works&hl=en&authuser=1&user=jXGgcRMAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="pdfs/cv.pdf">CV</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/urvashik.jpg" >
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Recent News</heading>
				      <ul>
                <li>
                  I was invited to give a talk at the <a href=http://ciir.cs.umass.edu/node/631>UMass Center for Intelligent Information Retrieval Talk Series</a>. You can watch <a href=https://www.youtube.com/watch?v=8f09iJKb6Ps>the recorded talk</a> on youtube.
                </li>
                <li>
                  I completed my PhD at Stanford in June 2021 and now work at Google AI. Check out <a href=https://stacks.stanford.edu/file/druid:st056pp9441/urvashi_thesis-augmented.pdf>my PhD thesis</a>!
                </li>
				      </ul>
              <a href="#Updates">Other Updates</a>.
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
				      <ul>
                <li>
                  <p>
                    <b><a href=https://stacks.stanford.edu/file/druid:st056pp9441/urvashi_thesis-augmented.pdf>Improving Neural Language Models with Black-Box Analysis and Generalization through Memorization.</a></b></br>
                    <b><u>Urvashi Khandelwal</u></b>.</br>
                     PhD Thesis, Stanford University, 2021.</br> 
                     [<a href=bibs/khandelwal2021improving.bib>bib</a>]
                  </p>
         	      </li>

        	      <li>
                  <p>
                    <b><a href=https://arxiv.org/pdf/2010.00710.pdf>Nearest Neighbor Machine Translation.</a></b></br>
                    <b><u>Urvashi Khandelwal</u></b>, Angela Fan, Dan Jurafsky, Luke Zettlemoyer and Mike Lewis.</br>
                     International Conference on Learning Representations (ICLR), 2021.</br> 
                     [<a href=bibs/khandelwal2021nearest.bib>bib</a>][<a href=https://github.com/urvashik/knnmt>code</a>][<a href=pdfs/knnmt_iclr.pdf>slides</a>][<a href=https://iclr.cc/virtual/2021/poster/2532>talk</a>]
                  </p>
         	      </li>


                        <li>
                        <p>
                            <b><a href=https://arxiv.org/pdf/2010.06595.pdf>With Little Power Comes Great Responsibility.</a></b></br>
                            Dallas Card, Peter Henderson, <b><u>Urvashi Khandelwal</u></b>, Robin Jia, Kyle Mahowald and Dan Jurafsky.<br>
                            Empirical Methods in Natural Language Processing (EMNLP), 2020.</br>
                            [<a href=bibs/card2020with.bib>bib</a>]
                        </p>
                        </li>

								<li>
                                <p>
                  <b><a href=https://www.pnas.org/content/pnas/early/2020/06/02/1907367117.full.pdf>Emergent Linguistic Structure in Artificial Neural
                    Networks Trained by Self-Supervision.</a></b></br>
                  Christopher D. Manning, Kevin Clark, John Hewitt, <b><u>Urvashi Khandelwal</u></b> and Omer Levy.</br>
                  Proceedings of the National Academy of Sciences (PNAS), 2020.</br>
                  [<a href=bibs/manning2020emergent.bib>bib</a>]
                  </p>
								</li>

        	      <li>
                  <p>
                    <b><a href=https://openreview.net/pdf?id=HklBjCEKvH>Generalization through Memorization: Nearest Neighbor Language Models.</a></b></br>
                    <b><u>Urvashi Khandelwal</u></b>, Omer Levy, Dan Jurafsky, Luke Zettlemoyer and Mike Lewis.</br>
                    International Conference on Learning Representations (ICLR), 2020.</br> 
                    [<a href=bibs/khandelwal2020generalization.bib>bib</a>][<a href=https://github.com/urvashik/knnlm>code</a>][<a href=pdfs/knnlm_iclr_final.pdf>slides</a>][<a href=https://iclr.cc/virtual_2020/poster_HklBjCEKvH.html>talk</a>]
                  </p>
         	      </li>
                

								<li>
                	<p>
                		<b><a href=https://nlp.stanford.edu/pubs/clark2019what.pdf>What does BERT look at? An Analysis of BERT's Attention.</a></b></br>
                    Kevin Clark, <b><u>Urvashi Khandelwal</u></b>, Omer Levy and Christopher D. Manning.</br>
                    BlackboxNLP@ACL, 2019.</br> 
                    <b style="color:#BC3C3C">Best Paper Award.</b></br>
                    [<a href=bibs/clark2019what.bib>bib</a>]
                	</p>
                </li>


                <li>
                	<p>
                		<b><a href=https://nlp.stanford.edu/pubs/clark2019bam.pdf>BAM! Born-Again Multi-Task Networks for Natural Language Understanding.</a></b></br>
                    Kevin Clark, Minh-Thang Luong, <b><u>Urvashi Khandelwal</u></b>, Christopher D. Manning and Quoc V. Le.</br>
                    Association for Computational Linguistics (ACL), 2019.</br>
                    [<a href=bibs/clark2019bam.bib>bib</a>]
                	</p>
                </li>

								<li>
                	<p>
                    <b><a href=https://arxiv.org/pdf/1905.08836.pdf>Sample Efficient Text Summarization Using a Single Pre-Trained Transformer.</a></b></br>
                    <b><u>Urvashi Khandelwal</u></b>, Kevin Clark, Dan Jurafsky, Lukasz Kaiser.</br>
                    ArXiv Preprint, 2019. Presented at WeCNLP 2019.</br>
                    [<a href=bibs/khandelwal2019sample.bib>bib</a>][<a href=https://github.com/tensorflow/tensor2tensor>code</a>][<a href=pdfs/wecnlp_poster_final.pdf>poster</a>]
                	</p>
                </li>

								<li>
                	<p>
                    <b><a href=https://arxiv.org/pdf/1805.04623.pdf>Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context.</a></b></br>
                    <b><u>Urvashi Khandelwal</u></b>, He He, Peng Qi, Dan Jurafsky.</br>
                    Association for Computational Linguistics (ACL), 2018.</br>
                    [<a href=bibs/khandelwal2018sharp.bib>bib</a>][<a href=https://github.com/urvashik/lm-context-analysis>code</a>][<a href=pdfs/acl2018_poster_final.pdf>poster</a>]
                	</p>
                </li>

								<li>
                  <p>
                    <b><a href=http://hanj.cs.illinois.edu/pdf/kdd14_xren.pdf>ClusCite: Effective Citation Recommendation by Information Network-Based Clustering.</a></b></br>
                    Xiang Ren, Jialu Liu, Xiao Yu, <b><u>Urvashi Khandelwal</u></b>, Quanquan Gu, Lidan. Wang, Jiawei Han.</br>
                    ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2014.
                  </p>
                </li>

                <li>
                  <p>
                    <b><a href=http://hanj.cs.illinois.edu/pdf/wsdm14_xyu.pdf>Personalized Entity Recommendation: A Heterogeneous Information Network Approach.</a></b></br>
                    Xiao Yu, Xiang Ren, Yizhou Sun, Quanquan Gu, Brad Sturt, <b><u>Urvashi Khandelwal</u></b>, Brandon Norick, Jiawei Han.<br>
                    International Conference on Web Search and Data Mining (WSDM), 2014.
                  </p>
                </li>

                <li>
                  <p>
                    <b><a href=http://hanj.cs.illinois.edu/pdf/recsys13_xyu.pdf>HeteRec: Entity Recommendation in Heterogeneous Information Networks with Implicit User Feedback.</a></b></br>
                    Xiao Yu, Xiang Ren, Yizhou Sun, Brad Sturt, <b><u>Urvashi Khandelwal</u></b>, Quanquan Gu, Brandon Norick, Jiawei Han.<br>
                    ACM Conference on Recommender Systems (RecSys), 2013.
                  </p>
                </li>

				      </ul>

            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <a id="Updates" style="color: inherit;
  text-decoration: inherit;"><heading>Invited Talks, Awards and Other Projects</heading></a>
				      <ul>
                <li>
                  I won a <a href=https://www.microsoft.com/en-us/research/academic-program/dissertation-grant/#!grant-recipients>Microsoft Research Dissertation Grant</a> in 2020, thanks MSR!
                </li>
								<li>
                  I was invited to speak at the <a href=https://nlp.berkeley.edu/2019/11/12/nov-18-urvashi-khandelwal-john-hewitt/>Berkeley NLP Seminar</a> about my work on Nearest Neighbor Language Models.</br>
                </li>
                <li>
                  Our paper, "<a href=https://nlp.stanford.edu/pubs/clark2019what.pdf>What does BERT look at? An Analysis of BERT's Attention</a>", won the best paper award at BlackboxNLP 2019!
                </li>
                <li>
                  I co-organized the <a href=https://aclanthology.org/volumes/W19-23/>Workshop on Methods for Optimizing and Evaluating Neural Language Generation (NeuralGen 2019)</a> co-located with NAACL'19 in Minneapolis.
                </li>
                <li>
                  I was invited to speak at the <a href=https://www.meetup.com/Bay-Area-Research-in-NLP-and-ML/events/259348080/>Bay Area Research in NLP and ML</a> Meetup in March, 2019.</br>[<a href=pdfs/meetup_march19.pdf>slides</a>]
                </li>
                <li>
                  In Winter 2019, I participated in a policy lab offered by the Stanford Law School called <i>Administering by Algorithm: Artificial Intelligence in the Regulatory State</i>. I was quoted in a <a href=https://news.stanford.edu/2019/02/28/policy-lab-explores-government-administers-algorithm/>Stanford News Service article</a> about it! We submitted a report to the Administrative Conference of the United States: <a href=https://www-cdn.law.stanford.edu/wp-content/uploads/2020/02/ACUS-AI-Report.pdf>Government by Algorithm: Artificial Intelligence in Federal Administrative Agencies</a>. 

                </li>

				      </ul>

            </td>
          </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p style="text-align:left;font-size:small;">
                This website template is from <a href=https://jonbarron.info/ style="text-align:right;font-size:small;">Jon Barron</a>.
              </p>
            </td>
          </tr>
        </tbody></table>


      </td>
    </tr>
  </table>
</body>

</html>
